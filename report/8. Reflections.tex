\section{Reflections}

In this section, we reflect on the process and result of our work.
We note some potential improvements that could be made to our tool, UPPAAL, based on our experience as newcomers.
We also discuss a few underspecifiations and assumptions of the original paper.
We reflect on our approach to modeling, what stumbling blocks took a lot of time for us and what approach we have found to be helpful when modeling swarm robotics algorithms.
Finally, we consider how our work may be improved and built upon.

\subsection{UPPAAL as a modeling tool}

We have benefited greatly from UPPAALs support for types, functions and templates.
These features made it much easier for us to translate our existing programming knowledge into modeling work - which is even more enhanced by the C-like syntax.
UPPAAL is tool in development, and many useful and user-friendly features have been added during recent versions.
The graphical editor and simulator also make it easy to inspect the models behavior which greatly simplifies debugging.

However, UPPAAL suffers from outdated, lacking and at times incorrect documentation.
There are official documentation pages, but many specifications, language idioms, and behaviors are distributed over presentations and tutorial written over multiple decades and major software versions â€“ there are even some rules in the language we have not found documented at all.
This leads to unexpected errors and inconsistent understanding of the rules of the language, which works counter to the purpose of formal verification.
The lacking documentation encourages a "try and see" approach to writing functions and models, where understanding is derived from empirical observation of the simulator.
Specifically for formal verification, it is crucial to be confident and exact in ones understanding of how changes to concrete syntax are translated into changes to the underlying model.
Therefore we argue that documentation is of special importance for a tool like UPPAAL.

Extending this observation, we found that while debugging and inspecting the model is easy, functions are not as well supported.
Because functions are called atomically during edge traversals in the simulator, there is no way of inspecting evaluation, which makes it hard to catch errors and verify behavior.

In our specific case, another limitation of the platform became apparent. 
UPPAAL has chosen to save systems as XML files and enforces a strict project structure.
This means that the entire project is saved as a single file, and is only comfortable to edit using UPPAALs own GUI editor.
Within the GUI, a project must have exactly one global declarations file, one declarations file per template, and one global declarations file.
Combined, this makes it cumbersome to collaborate on projects, as merge conflicts are inevitable and complex project cannot be broken down into components.
The ability to modularise a project in some way would make it possible to 

Finally, we note that UPPAAL currently has quite severe performance issues on newer MacOS machines that may stem from memory leaks.



\subsection{Correctness of our model}



\subsection{Realism of the algorithm}
\subsection{Relation between expected behaviour and model results}
In general, our results are very alike those presented in the algorithm article \parencite{AlgorithmPaper}. We find similar correlations between the various variables and the iterations needed for the network to converge and consent on a singular decision. In some cases, we have been unable to replicate the exact experiment setup that was used in the original work, but even with the approximate setups we get similar results. In terms of our fourth research question, we conclude that the numbers match exceedingly well. For both network dependency values, network size, options and external interference, the results from our synthesized automata model closely conform to the proposed values from the original article.
While network dependency value was found to correlate with the amount of iterations needed to finish a decision process, we have also found that it is not a perfect metric. Often during random tests and setup, we would encounter somewhat large variance in the iteration counts, even if two networks had the same network dependency value. This suggests that there are other factors of network topology that could be very relevant for how the algorithm performs, and certain structures that may cause long, unintended processes that impede and slow down the decision making process. We have not been able to pin down any other specific, significant structure, but we theorize that they exist, and leave the discovery to possible future work.
